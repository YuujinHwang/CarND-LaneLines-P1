{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This notebook contains __Writeup Template__\n",
    "\n",
    "\n",
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "\n",
    "## Project: **Finding Lane Lines on the Road** \n",
    "***\n",
    "In this project, you will use the tools you learned about in the lesson to identify lane lines on the road.  You can develop your pipeline on a series of individual images, and later apply the result to a video stream (really just a series of images). Check out the video clip \"raw-lines-example.mp4\" (also contained in this repository) to see what the output should look like after using the helper functions below. \n",
    "\n",
    "Once you have a result that looks roughly like \"raw-lines-example.mp4\", you'll need to get creative and try to average and/or extrapolate the line segments you've detected to map out the full extent of the lane lines.  You can see an example of the result you're going for in the video \"P1_example.mp4\".  Ultimately, you would like to draw just one line for the left side of the lane, and one for the right.\n",
    "\n",
    "In addition to implementing code, there is a brief writeup to complete. The writeup should be completed in a separate file, which can be either a markdown file or a pdf document. There is a [write up template](https://github.com/udacity/CarND-LaneLines-P1/blob/master/writeup_template.md) that can be used to guide the writing process. Completing both the code in the Ipython notebook and the writeup template will cover all of the [rubric points](https://review.udacity.com/#!/rubrics/322/view) for this project.\n",
    "\n",
    "---\n",
    "### Finding Lane Lines on the Road\n",
    "\n",
    "The goals / steps of this project are the following : \n",
    "\n",
    "* Make a pipeline that finds lane lines on the road\n",
    "* Reflect on your work in a written report\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe pipeline\n",
    "\n",
    "\n",
    "My pipeline use 3-step approach to find the lane lines.\n",
    "\n",
    "* Generate rough lines using canny edge detection with grayscaled image\n",
    "* Convert the edges into Hough space and fit the left and right line\n",
    "* Using color filtered image, determine the color of line (white/yellow)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1. Edge detection\n",
    "\n",
    "To find the lines from image, I used bulit-in/library functions introduced in the class.\n",
    "First, target imaged is converted into grayscale using helper function `grayscale`, and Canny edge detection is applied to specify edges.\n",
    "\n",
    "Tuning parameters for `GaussianBlur` and `Canny` from `cv2` as follows:\n",
    "```\n",
    "kernel_size = 7\n",
    "low_threshold = 70\n",
    "high_threshold = 150\n",
    "```\n",
    "\n",
    "#### Step 2. Finding lines with Hough Transform\n",
    "\n",
    "`HoughlinesP` function convert the points on xy-plane into lines on Hough space. Every single point will be represented as a straight line on Hough plane. The intersections of those lines means lines on xy-plane. Prams. and options used in this step are:\n",
    "```\n",
    "rho = 1 # \n",
    "theta = np.pi*1/180 \n",
    "threshold = 30    \n",
    "min_line_length = ysize/5 \n",
    "max_line_gap = 200  \n",
    "```\n",
    "\n",
    "`cv2.HoughlinesP` gives a number of lines which satisfy given constraints, so I need to filter out unnecessary lines that does'nt represent correct lane lines. In this project, a simple restriction of area and slope works fine. I set the region of interest, and get rid of lines looked horizontal. Next, distinguish between left and right line is done by check the sign of slope.\n",
    "\n",
    "```\n",
    "max_slope = 0.8*ysize/xsize\n",
    "lines_left=[]\n",
    "lines_right=[]\n",
    "for line in lines:\n",
    "    slope = (line[0][1]-line[0][3])/(line[0][0]-line[0][2])\n",
    "    if abs(slope) > max_slope:\n",
    "        if slope < 0:\n",
    "            line_ext=np.array([math.floor((ysize*0.6-(line[0][3]-slope*line[0][2]))/slope), math.floor(ysize*0.6),\n",
    "            math.floor((ysize-(line[0][3]-slope*line[0][2]))/slope), ysize ])\n",
    "            lines_left.append([line_ext])\n",
    "        else:\n",
    "            line_ext=np.array([math.floor((ysize*0.6-(line[0][3]-slope*line[0][2]))/slope), math.floor(ysize*0.6),\n",
    "            math.floor((ysize-(line[0][3]-slope*line[0][2]))/slope), ysize])\n",
    "            lines_right.append([line_ext])\n",
    "                    \n",
    "\n",
    "lines_left = np.asarray(lines_left)\n",
    "lines_right = np.asarray(lines_right)\n",
    "\n",
    "line_left = np.mean(lines_left, axis = 0)\n",
    "line_right = np.mean(lines_right, axis = 0)\n",
    "```\n",
    "\n",
    "#### Step 3. Check the color of line\n",
    "\n",
    "To separate _YELLOW_ lines from white line, the color selection method is used. The pure yellow is `[255 255 0]` in rgb, that means checking wheter the line contains blue colors or not can give us clues. To make it practical, the blue color thresholded image is processed into set of lines with step 1 and 2.\n",
    "And then, by trying to match left and right lines from step 2 with blue lines, the yellow colored lines can be detected.\n",
    "\n",
    "```\n",
    "    flag_left=0\n",
    "    for blueline in lines_blue:\n",
    "        for x1b,y1b,x2b,y2b in blueline:\n",
    "            blueslope = (y2b-y1b) / (x2b-x1b)\n",
    "            bluedist = (abs(-slope_left*x1b+y1b-offset_left) + \n",
    "            abs(-slope_left*x2b+y2b-offset_left)) / math.sqrt(slope_left**2 + 1)\n",
    "            if (abs(blueslope)-abs(slope_left)<0.05):\n",
    "                if bluedist < 30:\n",
    "                    flag_left=1\n",
    "                    break\n",
    "        if flag_left==1:\n",
    "            break\n",
    "            \n",
    "    if flag_left==1:\n",
    "        cv2_line(line_image_test, (line_selected[0][0][0], line_selected[0][0][1]), \n",
    "        (line_selected[0][0][2], line_selected[0][0][3]), (255,0,0),10)\n",
    "    else:\n",
    "        cv2_line(line_image_test, (line_selected[0][0][0], line_selected[0][0][1]), \n",
    "        (line_selected[0][0][2], line_selected[0][0][3]), (255,255,0),10)\n",
    "\n",
    "            \n",
    "    flag_right=0\n",
    "    for blueline in lines_blue:\n",
    "        for x1b,y1b,x2b,y2b in blueline:\n",
    "            blueslope = (y2b-y1b) / (x2b-x1b)\n",
    "            bluedist = (abs(-slope_right*x1b+y1b-offset_right) + \n",
    "            abs(-slope_right*x2b+y2b-offset_right)) / math.sqrt(slope_right**2 + 1)\n",
    "            if (abs(blueslope)-abs(slope_right)<0.05):\n",
    "                if bluedist < 30:\n",
    "                    flag_right=1\n",
    "                    break            \n",
    "        if flag_right==1:\n",
    "            break\n",
    "            \n",
    "    if flag_right==1:\n",
    "        cv2_line(line_image_test, (line_selected[1][0][0], line_selected[1][0][1]), \n",
    "        (line_selected[1][0][2], line_selected[1][0][3]), (255,0,0),10)\n",
    "    else:\n",
    "        cv2_line(line_image_test, (line_selected[1][0][0], line_selected[1][0][1]), \n",
    "        (line_selected[1][0][2], line_selected[1][0][3]), (255,255,0),10)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential shortcomings\n",
    "\n",
    "* Thresholds of edge detection is fixed. Detecting performance might varying with the contrast of given image\n",
    "* Curved line cannot be detected, since `HoughlineP` gives straight lines.\n",
    "* Splitted brigtness gives wrong result.\n",
    "* __(IMPORTANT) Incoding video stucks when the pipeline gives no lines to draw__, and this is from `cv2.line` only can handle nonzero type input. Exception handling must be required, but this time it remains as a bug due to my lack of python experience. \n",
    "    - because of this problem, extra.mp4 stops in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible improvements\n",
    "\n",
    "* Considering original quality of image, like brightness, contrast, color space, etc., can make the performance better.\n",
    "* Using previous state of detected line makes the pipeline more stable. (like many causal filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "#importing some useful packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 16,9 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image is: <class 'numpy.ndarray'> with dimesions: (720, 1280, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6cde674860>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading in an image\n",
    "#image = mpimg.imread('test_images/solidWhiteRight.jpg')\n",
    "image = mpimg.imread('challenge.jpg')\n",
    "\n",
    "#printing out some stats and plotting\n",
    "print('This image is:', type(image), 'with dimesions:', image.shape)\n",
    "plt.imshow(image)  \n",
    "\n",
    "# if you wanted to show a single color channel image called 'gray', for example, call as plt.imshow(gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for Lane Detection Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Below are some helper functions to help get you started. They should look familiar from the lesson!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def grayscale(img):\n",
    "    \"\"\"Applies the Grayscale transform\n",
    "    This will return an image with only one color channel\n",
    "    but NOTE: to see the returned image as grayscale\n",
    "    (assuming your grayscaled image is called 'gray')\n",
    "    you should call plt.imshow(gray, cmap='gray')\"\"\"\n",
    "    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Or use BGR2GRAY if you read an image with cv2.imread()\n",
    "    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "def canny(img, low_threshold, high_threshold):\n",
    "    \"\"\"Applies the Canny transform\"\"\"\n",
    "    return cv2.Canny(img, low_threshold, high_threshold)\n",
    "\n",
    "def gaussian_blur(img, kernel_size):\n",
    "    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)   \n",
    "    \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "\n",
    "def draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n",
    "    \"\"\"\n",
    "    I did not use draw_lines function.\n",
    "    All the features are included in \"process_image\"\n",
    "    \"\"\"\n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "    \"\"\"\n",
    "    `img` should be the output of a Canny transform.\n",
    "        \n",
    "    Returns an image with hough lines drawn.\n",
    "    \"\"\"\n",
    "    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "    draw_lines(line_img, lines)\n",
    "    return line_img\n",
    "\n",
    "# Python 3 has support for cool math symbols.\n",
    "\n",
    "def weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n",
    "    \"\"\"\n",
    "    `img` is the output of the hough_lines(), An image with lines drawn on it.\n",
    "    Should be a blank image (all black) with lines drawn on it.\n",
    "    \n",
    "    `initial_img` should be the image before any processing.\n",
    "    \n",
    "    The result image is computed as follows:\n",
    "    \n",
    "    initial_img * α + img * β + λ\n",
    "    NOTE: initial_img and img must be the same shape!\n",
    "    \"\"\"\n",
    "    return cv2.addWeighted(initial_img, α, img, β, λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Images\n",
    "\n",
    "Build your pipeline to work on the images in the directory \"test_images\"  \n",
    "**You should make sure your pipeline works well on these images before you try the videos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['solidYellowCurve2.jpg',\n",
       " 'solidYellowLeft.jpg',\n",
       " 'solidWhiteCurve.jpg',\n",
       " 'solidYellowCurve.jpg',\n",
       " 'whiteCarLaneSwitch.jpg',\n",
       " 'solidWhiteRight.jpg']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"test_images/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Lane Finding Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    # NOTE: The output you return should be a color image (3 channel) for processing video below\n",
    "    # TODO: put your pipeline here,\n",
    "    # you should return the final output (image where lines are drawn on lanes)\n",
    "    \n",
    "    \n",
    "    # TODO: Build your pipeline that will draw lane lines on the test_images\n",
    "    # then save them to the test_images directory.\n",
    "\n",
    "\n",
    "    # Define image properties\n",
    "    \n",
    "    ysize = image.shape[0]\n",
    "    xsize = image.shape[1]\n",
    "    left_bottom = [0, ysize]\n",
    "    right_bottom = [xsize, ysize]\n",
    "    apex = [xsize/2, ysize/2]\n",
    "\n",
    "\n",
    "    # Define the area of interest from the original image\n",
    "    fit_left = np.polyfit((left_bottom[0], apex[0]), (left_bottom[1], apex[1]), 1)\n",
    "    fit_right = np.polyfit((right_bottom[0], apex[0]), (right_bottom[1], apex[1]), 1)\n",
    "    fit_bottom = np.polyfit((left_bottom[0], right_bottom[0]), (left_bottom[1], right_bottom[1]), 1)\n",
    "\n",
    "    XX, YY = np.meshgrid(np.arange(0,xsize), np.arange(0,ysize))\n",
    "    region_thresholds = (YY > (XX*fit_left[0] + fit_left[1])) &\\\n",
    "                        (YY > (XX*fit_right[0] + fit_right[1])) &\\\n",
    "                        (YY < (XX*fit_bottom[0] + fit_bottom[1]))\n",
    "            \n",
    "    # Check the selected region\n",
    "    plt.imshow(image)\n",
    "    x = [left_bottom[0], right_bottom[0], apex[0], left_bottom[0]]\n",
    "    y = [left_bottom[1], right_bottom[1], apex[1], left_bottom[1]]\n",
    "    # Find mean value of RGBs\n",
    "    mean_rgb = 0\n",
    "\n",
    "    for i in range(xsize):\n",
    "        for j in range(ysize):\n",
    "            mean_rgb = mean_rgb + sum(image[j, i])*region_thresholds[j, i]\n",
    "\n",
    "    mean_rgb=mean_rgb//(3*np.sum(region_thresholds))\n",
    "\n",
    "    # Define color selection criteria\n",
    "    red_threshold = 2.2*mean_rgb;\n",
    "    green_threshold = 2.2*mean_rgb;\n",
    "    blue_threshold = 2.2*mean_rgb;\n",
    "\n",
    "    rgb_threshold = [ red_threshold, green_threshold, blue_threshold ]\n",
    "    color_thresholds = [ image[:,:,0] < rgb_threshold[0], image[:,:,1] < rgb_threshold[1], image[:,:,2] < rgb_threshold[2] ]\n",
    "\n",
    "    # Mask color and region\n",
    "    red_select = np.copy(image)\n",
    "    green_select = np.copy(image)\n",
    "    blue_select = np.copy(image)\n",
    "\n",
    "    red_select[color_thresholds[0] | ~region_thresholds ] = [0, 0, 0]\n",
    "    green_select[color_thresholds[1] | ~region_thresholds ] = [0, 0, 0]\n",
    "    blue_select[color_thresholds[2] | ~region_thresholds ] = [0, 0, 0]\n",
    "\n",
    "    red_select=red_select[:,:,0]\n",
    "    green_select=green_select[:,:,1]\n",
    "    blue_select=blue_select[:,:,2]\n",
    "\n",
    "\n",
    "\n",
    "    # Convert to grayscale\n",
    "\n",
    "    gray = grayscale(image)\n",
    "\n",
    "    # Define Gaussian smoothing & Canny parameters\n",
    "\n",
    "    kernel_size = 7\n",
    "    blur_gray = cv2.GaussianBlur(gray, (kernel_size, kernel_size), 0)\n",
    "\n",
    "    low_threshold = 70\n",
    "    high_threshold = 150\n",
    "    edges  = cv2.Canny(blur_gray, low_threshold, high_threshold)\n",
    "\n",
    "    edge_select=np.copy(edges)\n",
    "    edge_select[ ~region_thresholds ] = 0\n",
    "\n",
    "    # Apply Hough Transform to find Lane Lines\n",
    "    # Define the Hough transform parameters\n",
    "    # Make a blank the same size as our image to draw on\n",
    "    rho = 1 # distance resolution in pixels of the Hough grid\n",
    "    theta = np.pi*1/180 # angular resolution in radians of the Hough grid\n",
    "    threshold = 30    # minimum number of votes (intersections in Hough grid cell)\n",
    "    min_line_length = ysize/5 #minimum number of pixels making up a line\n",
    "    max_line_gap = 200   # maximum gap in pixels between connectable line segments\n",
    "    line_image = np.copy(image)*0 # creating a blank to draw lines on\n",
    "    line_image_red = np.copy(image)*0\n",
    "    line_image_green = np.copy(image)*0\n",
    "    line_image_blue = np.copy(image)*0\n",
    "\n",
    "    # Run Hough on edge detected image\n",
    "    # Output \"lines\" is an array containing endpoints of detected line segments\n",
    "    lines = cv2.HoughLinesP(edge_select, rho, theta, threshold, np.array([]),\n",
    "                                min_line_length, max_line_gap)\n",
    "\n",
    "    # Iterate over the output \"lines\" and draw lines on a blank image\n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(line_image,(x1,y1),(x2,y2),(255,0,0),10)\n",
    "\n",
    "    # Create a \"color\" binary image to combine with line image\n",
    "    color_edges = np.dstack((edge_select, edge_select, edge_select)) \n",
    "\n",
    "    # Draw the lines on the edge image\n",
    "    lines_edges = cv2.addWeighted(color_edges, 0.8, line_image, 1, 0) \n",
    "\n",
    "\n",
    "    lines_red = cv2.HoughLinesP(red_select, rho, theta, threshold, np.array([]),\n",
    "                                min_line_length, max_line_gap)\n",
    "    # Iterate over the output \"lines\" and draw lines on a blank image\n",
    "    for line in lines_red:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(line_image_red,(x1,y1),(x2,y2),(255,0,0),10)\n",
    "\n",
    "    # Create a \"color\" binary image to combine with line image\n",
    "    color_edges = np.dstack((red_select, red_select, red_select)) \n",
    "\n",
    "    # Draw the lines on the edge image\n",
    "    lines_edges_red = cv2.addWeighted(color_edges, 0.8, line_image_red, 1, 0) \n",
    "\n",
    "    lines_green = cv2.HoughLinesP(green_select, rho, theta, threshold, np.array([]),\n",
    "                                min_line_length, max_line_gap)\n",
    "    # Iterate over the output \"lines\" and draw lines on a blank image\n",
    "    for line in lines_green:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(line_image_green,(x1,y1),(x2,y2),(255,0,0),10)\n",
    "\n",
    "    # Create a \"color\" binary image to combine with line image\n",
    "    color_edges = np.dstack((green_select, green_select, green_select)) \n",
    "\n",
    "    # Draw the lines on the edge image\n",
    "    lines_edges_green = cv2.addWeighted(color_edges, 0.8, line_image_green, 1, 0) \n",
    "\n",
    "    lines_blue = cv2.HoughLinesP(blue_select, rho, theta, threshold, np.array([]),\n",
    "                                min_line_length, max_line_gap)\n",
    "    # Iterate over the output \"lines\" and draw lines on a blank image\n",
    "    for line in lines_blue:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(line_image_blue,(x1,y1),(x2,y2),(255,0,0),10)\n",
    "\n",
    "    # Create a \"color\" binary image to combine with line image\n",
    "    color_edges = np.dstack((blue_select, blue_select, blue_select)) \n",
    "\n",
    "    # Draw the lines on the edge image\n",
    "    lines_edges_blue = cv2.addWeighted(color_edges, 0.8, line_image_blue, 1, 0) \n",
    "\n",
    "    max_slope = 0.8*ysize/xsize\n",
    "\n",
    "    lines_left=[]\n",
    "    lines_right=[]\n",
    "    for line in lines:\n",
    "        slope = (line[0][1]-line[0][3])/(line[0][0]-line[0][2])\n",
    "        if abs(slope) > max_slope:\n",
    "            if slope < 0:\n",
    "                line_ext=np.array([math.floor((ysize*0.6-(line[0][3]-slope*line[0][2]))/slope), math.floor(ysize*0.6), math.floor((ysize-(line[0][3]-slope*line[0][2]))/slope), ysize ])\n",
    "                lines_left.append([line_ext])\n",
    "            else:\n",
    "                line_ext=np.array([math.floor((ysize*0.6-(line[0][3]-slope*line[0][2]))/slope), math.floor(ysize*0.6), math.floor((ysize-(line[0][3]-slope*line[0][2]))/slope), ysize])\n",
    "                lines_right.append([line_ext])\n",
    "                    \n",
    "\n",
    "    lines_left = np.asarray(lines_left)\n",
    "    lines_right = np.asarray(lines_right)\n",
    "\n",
    "    line_left = np.mean(lines_left, axis = 0)\n",
    "    line_right = np.mean(lines_right, axis = 0)\n",
    "\n",
    "    slope_left = (line_left[0][1]-line_left[0][3])/(line_left[0][0]-line_left[0][2])\n",
    "    slope_right = (line_right[0][1]-line_right[0][3])/(line_right[0][0]-line_right[0][2])\n",
    "    offset_left = math.floor((line_left[0][3]-slope_left*line_left[0][2]))\n",
    "    offset_right = math.floor((line_right[0][3]-slope_right*line_right[0][2]))\n",
    "\n",
    "    line_params = [ slope_left, slope_right, offset_left, offset_right]\n",
    "\n",
    "    line_selected=np.asarray([line_left.astype(int), line_right.astype(int)])\n",
    "\n",
    "    line_image_test=np.copy(image)\n",
    "\n",
    "    flag_left=0\n",
    "    for blueline in lines_blue:\n",
    "        for x1b,y1b,x2b,y2b in blueline:\n",
    "            blueslope = (y2b-y1b) / (x2b-x1b)\n",
    "            bluedist = (abs(-slope_left*x1b+y1b-offset_left) + abs(-slope_left*x2b+y2b-offset_left)) / math.sqrt(slope_left**2 + 1)\n",
    "            if (abs(blueslope)-abs(slope_left)<0.05):\n",
    "                if bluedist < 30:\n",
    "                    flag_left=1\n",
    "                    break\n",
    "        if flag_left==1:\n",
    "            break\n",
    "            \n",
    "    if flag_left==1:\n",
    "        cv2.line(line_image_test, (line_selected[0][0][0], line_selected[0][0][1]), (line_selected[0][0][2], line_selected[0][0][3]), (255,0,0),10)\n",
    "    else:\n",
    "        cv2.line(line_image_test, (line_selected[0][0][0], line_selected[0][0][1]), (line_selected[0][0][2], line_selected[0][0][3]), (255,255,0),10)\n",
    "\n",
    "            \n",
    "    flag_right=0\n",
    "    for blueline in lines_blue:\n",
    "        for x1b,y1b,x2b,y2b in blueline:\n",
    "            blueslope = (y2b-y1b) / (x2b-x1b)\n",
    "            bluedist = (abs(-slope_right*x1b+y1b-offset_right) + abs(-slope_right*x2b+y2b-offset_right)) / math.sqrt(slope_right**2 + 1)\n",
    "            if (abs(blueslope)-abs(slope_right)<0.05):\n",
    "                if bluedist < 30:\n",
    "                    flag_right=1\n",
    "                    break            \n",
    "        if flag_right==1:\n",
    "            break\n",
    "            \n",
    "    if flag_right==1:\n",
    "        cv2.line(line_image_test, (line_selected[1][0][0], line_selected[1][0][1]), (line_selected[1][0][2], line_selected[1][0][3]), (255,0,0),10)\n",
    "    else:\n",
    "        cv2.line(line_image_test, (line_selected[1][0][0], line_selected[1][0][1]), (line_selected[1][0][2], line_selected[1][0][3]), (255,255,0),10)\n",
    "\n",
    "    lines_edges_test = cv2.addWeighted(color_edges, 0.8, line_image_test, 1, 0) \n",
    "    plt.imshow(lines_edges_test)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    return lines_edges_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the one with the solid white lane on the right first ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2_line' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5a41aa557f47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwhite_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'white.mp4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclip1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"solidWhiteRight.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwhite_clip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#NOTE: this function expects color images!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time white_clip.write_videofile(white_output, audio=False)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yunjin/anaconda2/envs/udacity/lib/python3.5/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36mfl_image\u001b[0;34m(self, image_func, apply_to)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mby\u001b[0m \u001b[0manother\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \"\"\"\n\u001b[0;32m--> 513\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0;31m# --------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yunjin/anaconda2/envs/udacity/lib/python3.5/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mfl\u001b[0;34m(self, fun, apply_to, keep_duration)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m#mf = copy(self.make_frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mnewclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_make_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-178>\u001b[0m in \u001b[0;36mset_make_frame\u001b[0;34m(self, mf)\u001b[0m\n",
      "\u001b[0;32m/home/yunjin/anaconda2/envs/udacity/lib/python3.5/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36moutplace\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m\"\"\" Applies f(clip.copy(), *a, **k) and returns clip.copy()\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mnewclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnewclip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yunjin/anaconda2/envs/udacity/lib/python3.5/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36mset_make_frame\u001b[0;34m(self, mf)\u001b[0m\n\u001b[1;32m    651\u001b[0m         \"\"\"\n\u001b[1;32m    652\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-135>\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n",
      "\u001b[0;32m/home/yunjin/anaconda2/envs/udacity/lib/python3.5/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(f, *a, **kw)\u001b[0m\n\u001b[1;32m     87\u001b[0m         new_kw = {k: fun(v) if k in varnames else v\n\u001b[1;32m     88\u001b[0m                  for (k,v) in kw.items()}\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yunjin/anaconda2/envs/udacity/lib/python3.5/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yunjin/anaconda2/envs/udacity/lib/python3.5/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m#mf = copy(self.make_frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mnewclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_make_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yunjin/anaconda2/envs/udacity/lib/python3.5/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(gf, t)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mby\u001b[0m \u001b[0manother\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \"\"\"\n\u001b[0;32m--> 513\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0;31m# --------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-623a61225b66>\u001b[0m in \u001b[0;36mprocess_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mcv2_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# Create a \"color\" binary image to combine with line image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2_line' is not defined"
     ]
    }
   ],
   "source": [
    "white_output = 'white.mp4'\n",
    "clip1 = VideoFileClip(\"solidWhiteRight.mp4\")\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the video inline, or if you prefer find the video in your filesystem (should be in the same directory) and play it in your video player of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 0))\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'white_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a4e239a08e4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;34m<\u001b[0m\u001b[0msource\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{0}\"\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \"\"\".format(white_output))\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'white_output' is not defined"
     ]
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the one with the solid yellow lane on the left. This one's more tricky!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yellow_output = 'yellow.mp4'\n",
    "clip2 = VideoFileClip('solidYellowLeft.mp4')\n",
    "yellow_clip = clip2.fl_image(process_image)\n",
    "%time yellow_clip.write_videofile(yellow_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(yellow_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writeup and Submission\n",
    "\n",
    "If you're satisfied with your video outputs, it's time to make the report writeup in a pdf or markdown file. Once you have this Ipython notebook ready along with the writeup, it's time to submit for review! Here is a [link](https://github.com/udacity/CarND-LaneLines-P1/blob/master/writeup_template.md) to the writeup template file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optional Challenge\n",
    "\n",
    "Try your lane finding pipeline on the video below.  Does it still work?  Can you figure out a way to make it more robust?  If you're up for the challenge, modify your pipeline so it works with this video and submit it along with the rest of your project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "challenge_output = 'extra.mp4'\n",
    "clip2 = VideoFileClip('challenge.mp4')\n",
    "challenge_clip = clip2.fl_image(process_image)\n",
    "%time challenge_clip.write_videofile(challenge_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(challenge_output))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:udacity]",
   "language": "python",
   "name": "conda-env-udacity-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
